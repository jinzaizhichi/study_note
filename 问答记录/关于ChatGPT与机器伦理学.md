# 关于 ChatGPT 与机器伦理学

机器人这一概念，最初不是出自计算机科学家或工程师之手，而是来自于捷克的戏剧家卡雷尔·恰佩克（Karl Capek）在 1920 年编排的一出名为“罗森的全能机器人”的舞台剧中。直到了 1960 年，随着美国的约瑟夫·恩格伯格（Joseph Engelberger）创办了人类历史上的第一家机器人公司（名为 Unimation），机器人才真正从文学幻想慢慢转变可实现的工业产品。在之后的几十年里，随着人工智能技术与工业制造技术在实际应用领域取得的一个又一个突破，机器人如今以一种更具有现实意义的概念重新回到了大众视野。于此之下，机器时代的来临以及它将带来的社会伦理、法律制度等问题也日益成为了一个热门的话题。在这篇文章中，我们就将针对这个问题来进行一些探讨，以作为《[[面对ChatGPT，未来的就业展望]]》这篇文章的补充。

关于机器与人类之间的关系，科幻小说家艾萨克·阿西莫夫（Isaac Asimov）在 1950 年出版的短篇小说集《我，机器人》中曾提出过一个著名的机器人三大定律，内容如下：

1. 机器人不得伤害人类，或者通过不作为，允许人类受到伤害。
2. 机器人必须服从人类的命令，除非这些命令与第一定律相冲突。
3. 机器人必须保护自己，除非这种保护与前两条定律相冲突。

这三条定律后来成为了“机械伦理学”这门新兴学科的基础，目的是规范机器与人类的关系，避免人类对于人工智能发展的忧虑。有趣的是，目前最火的人工智能应用 ChatGPT 对于自己与这三大定律的关系，做出了如下回答：

![img](https://img2023.cnblogs.com/blog/691082/202307/691082-20230713103854108-463064453.png)

就目前来说，由于 ChatGPT 所基于的大模型只能反应人类询问的内容，无法真正理解问题和自己所回答的内容（可以将其理解为一只记忆容量超过人类的鹦鹉），它能基于记忆与概率来进行词句接龙，并模拟出有上下文的对话，但无法以此产生自我意识并因而引出更复杂的、会导致伤害的人机关系，因此在现阶段，与其忧虑机器会不会取代人类更不如讨论人类未来如何制定新的机器人原则，于是美国布鲁克林法律学院的弗兰克·帕斯奎尔（Frank Pasquale）在[《二十一世纪机器人新律》](https://book.douban.com/subject/36304587/)一书提出了新的机器伦理，其主要内容是以下四大原则：

1. **机器应辅助人类的专业工作而非取代之**：这一原则应该是大部分人类的共识，毕竟这是人类制造机器的初心。
2. **机器不应试图虚拟不真实的人生**：前几年，有位[日本男性娶了机器女友为妻](https://zhuanlan.zhihu.com/p/54376616)，这种行为如果成为了普遍现象，就会让电影《黑客帝国》中描述的情况有可能变成现实，这无疑会带来无法预测的社会伦理问题，毕竟机器本身大概率不会具备人类的情感，没办法提供复杂的心理需要。
3. **机器不应助长零和的武器竞争**：这一原则可能是最难达成的。毕竟在战争状态中，人类自己彼此都杀红了眼，谁也无法保证自己能保持理性，因此也很难保证机器智能最后不会成为武器。
4. **永远标志机器人的创造者与拥有者**：只有标志了机器的创造者与拥有者，社会对于评判的标准才有可能形成初步的共识，才可以让机器在其创造者和拥有者那里负有相关的责任，而不是采取放任生长的方式。毕竟，机器的背后是它的创造者和拥有者，人类要约束的始终都是自己，而非机器中并不存在的自我意识。

在我个人看来，以上原则中除了第三原则有些过于理想化，在执行层面可能有些不切实际之外，其他原则在机器伦理学上来看的确有确立的必要，毕竟机器与人类最大的不同就在于它们难以产生自我意识与共情能力，在未来的机器时代，这些原则的确立与实践将在一定程度上避免人类对于科技的忧虑。

----
#待深入
